"""LLM Transparency Utilities - å†…éƒ¨å‹•ä½œã®å¯è¦–åŒ–

LLMã®å‹•ä½œã‚’é€æ˜åŒ–ã—ã€å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã§ã‚ã‚‹ã“ã¨ã‚’
æ¤œè¨¼å¯èƒ½ã«ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é›†
"""

import functools
import hashlib
import time
from collections.abc import Callable
from typing import Any

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage


class TransparentLLM:
    """LLMã®å‹•ä½œã‚’é€æ˜åŒ–ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, llm: BaseChatModel, enable_verification: bool = True):
        self.llm = llm
        self.enable_verification = enable_verification
        self.call_history: list[dict[str, Any]] = []
        self._call_count = 0

    async def ainvoke(self, prompt: str, **kwargs) -> BaseMessage:
        """é€æ˜æ€§ã‚’ç¢ºä¿ã—ãŸLLMå‘¼ã³å‡ºã—"""
        call_id = f"call_{self._call_count:04d}"
        self._call_count += 1

        # å‘¼ã³å‡ºã—è¨˜éŒ²é–‹å§‹
        call_record = {
            "call_id": call_id,
            "timestamp": time.time(),
            "prompt": prompt,
            "prompt_hash": hashlib.sha256(prompt.encode()).hexdigest()[:16],
            "kwargs": kwargs,
        }

        if self.enable_verification:
            print(f"\nğŸ” LLM Call #{call_id}")
            print(f"   Prompt hash: {call_record['prompt_hash']}")
            print(f"   Prompt length: {len(prompt)} chars")

        # å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—
        start_time = time.time()
        try:
            response = await self.llm.ainvoke(prompt, **kwargs)
            latency_ms = int((time.time() - start_time) * 1000)

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¨˜éŒ²
            call_record.update(
                {
                    "response": response.content,
                    "response_hash": hashlib.sha256(response.content.encode()).hexdigest()[:16],
                    "latency_ms": latency_ms,
                    "success": True,
                }
            )

            if self.enable_verification:
                print(f"   Response hash: {call_record['response_hash']}")
                print(f"   Latency: {latency_ms}ms")
                print(f"   Response length: {len(response.content)} chars")

        except Exception as e:
            call_record.update(
                {
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "latency_ms": int((time.time() - start_time) * 1000),
                    "success": False,
                }
            )

            if self.enable_verification:
                print(f"   âŒ Error: {type(e).__name__}")

            raise

        finally:
            self.call_history.append(call_record)

        return response

    def get_verification_summary(self) -> dict[str, Any]:
        """å‘¼ã³å‡ºã—å±¥æ­´ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.call_history:
            return {"total_calls": 0, "summary": "No calls made"}

        successful_calls = [c for c in self.call_history if c.get("success", False)]
        failed_calls = [c for c in self.call_history if not c.get("success", False)]

        latencies = [c["latency_ms"] for c in successful_calls if "latency_ms" in c]
        avg_latency = sum(latencies) / len(latencies) if latencies else 0

        return {
            "total_calls": len(self.call_history),
            "successful_calls": len(successful_calls),
            "failed_calls": len(failed_calls),
            "average_latency_ms": int(avg_latency),
            "min_latency_ms": min(latencies) if latencies else 0,
            "max_latency_ms": max(latencies) if latencies else 0,
            "unique_prompts": len(
                {c.get("prompt_hash", "") for c in self.call_history if "prompt_hash" in c}
            ),
            "unique_responses": len(
                {c.get("response_hash", "") for c in successful_calls if "response_hash" in c}
            ),
        }


def verify_llm_call(func: Callable) -> Callable:
    """LLMå‘¼ã³å‡ºã—ã‚’æ¤œè¨¼ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""

    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        print(f"\nğŸ” Verifying LLM call in {func.__name__}")
        start_time = time.time()

        # é–¢æ•°å®Ÿè¡Œå‰ã®çŠ¶æ…‹ã‚’è¨˜éŒ² (for debugging if needed)
        # pre_state recorded but not used in current implementation

        try:
            # å®Ÿéš›ã®é–¢æ•°å®Ÿè¡Œ
            result = await func(*args, **kwargs)

            # å®Ÿè¡Œå¾Œã®æ¤œè¨¼
            execution_time = time.time() - start_time
            post_state = {
                "execution_time_ms": int(execution_time * 1000),
                "result_type": type(result).__name__,
                "result_hash": (
                    hashlib.sha256(str(result).encode()).hexdigest()[:16] if result else "none"
                ),
            }

            print("âœ… Verification passed:")
            print(f"   Execution time: {post_state['execution_time_ms']}ms")
            print(f"   Result type: {post_state['result_type']}")
            print(f"   Result hash: {post_state['result_hash']}")

            # ç•°å¸¸æ¤œå‡º
            if execution_time < 0.01:  # 10msæœªæº€ã¯ç–‘ã‚ã—ã„
                print("âš ï¸  Warning: Execution was suspiciously fast")

            return result

        except Exception as e:
            print(f"âŒ Verification failed: {type(e).__name__}: {str(e)}")
            raise

    return wrapper


class LLMCallTracker:
    """ãƒ†ã‚¹ãƒˆç”¨ã®LLMå‘¼ã³å‡ºã—ãƒˆãƒ©ãƒƒã‚«ãƒ¼"""

    def __init__(self):
        self.reset()

    def reset(self):
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.calls: list[dict[str, Any]] = []
        self.total_tokens = 0
        self.total_cost = 0.0

    def track_call(
        self,
        prompt: str,
        response: str,
        model: str,
        latency_ms: int,
        tokens: dict[str, int] | None = None,
    ):
        """LLMå‘¼ã³å‡ºã—ã‚’è¨˜éŒ²"""
        call_data = {
            "timestamp": time.time(),
            "prompt": prompt,
            "response": response,
            "model": model,
            "latency_ms": latency_ms,
            "prompt_tokens": tokens.get("prompt_tokens", 0) if tokens else 0,
            "completion_tokens": tokens.get("completion_tokens", 0) if tokens else 0,
            "total_tokens": tokens.get("total_tokens", 0) if tokens else 0,
        }

        # ã‚³ã‚¹ãƒˆæ¨å®šï¼ˆGemini Flash: $0.075 per 1M input, $0.30 per 1M outputï¼‰
        if tokens:
            input_cost = (tokens.get("prompt_tokens", 0) / 1_000_000) * 0.075
            output_cost = (tokens.get("completion_tokens", 0) / 1_000_000) * 0.30
            call_data["estimated_cost"] = input_cost + output_cost
            self.total_cost += call_data["estimated_cost"]
            self.total_tokens += tokens.get("total_tokens", 0)

        self.calls.append(call_data)

    def get_summary(self) -> dict[str, Any]:
        """ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.calls:
            return {"message": "No LLM calls tracked"}

        latencies = [c["latency_ms"] for c in self.calls]

        return {
            "total_calls": len(self.calls),
            "total_tokens": self.total_tokens,
            "estimated_total_cost": f"${self.total_cost:.6f}",
            "average_latency_ms": int(sum(latencies) / len(latencies)),
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies),
            "models_used": list({c["model"] for c in self.calls}),
        }

    def print_detailed_report(self):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
        print("\n" + "=" * 60)
        print("LLM Call Tracking Report")
        print("=" * 60)

        for i, call in enumerate(self.calls, 1):
            print(f"\nCall #{i}:")
            print(f"  Model: {call['model']}")
            print(f"  Latency: {call['latency_ms']}ms")
            print(f"  Tokens: {call['total_tokens']}")
            if "estimated_cost" in call:
                print(f"  Cost: ${call['estimated_cost']:.6f}")
            print(f"  Prompt preview: {call['prompt'][:50]}...")
            print(f"  Response preview: {call['response'][:50]}...")

        summary = self.get_summary()
        print("\n" + "-" * 60)
        print("Summary:")
        for key, value in summary.items():
            print(f"  {key}: {value}")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
global_llm_tracker = LLMCallTracker()


def get_llm_tracker() -> LLMCallTracker:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«LLMãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’å–å¾—"""
    return global_llm_tracker

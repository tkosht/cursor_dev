<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>企業情報クローリングシステム 基本設計書</title>
  <!-- Mermaid.js -->
  <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({
      startOnLoad: true
    });
  </script>
  <style>
    body {
      font-family: sans-serif;
      margin: 2em;
      line-height: 1.6;
    }
    h1, h2, h3 {
      color: #333;
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    ul {
      margin-bottom: 1em;
    }
    li {
      margin-bottom: 0.3em;
    }
    table {
      border-collapse: collapse;
      margin-bottom: 1em;
    }
    table, th, td {
      border: 1px solid #ccc;
    }
    th, td {
      padding: 0.5em;
      text-align: left;
    }
    code {
      background-color: #f4f4f4;
      padding: 0.2em 0.4em;
      margin: 0 0.2em;
      font-size: 0.95em;
    }
    .note {
      color: #999;
      font-size: 0.9em;
    }
    .mermaid {
      border: 1px solid #ccc;
      padding: 1em;
      margin: 1em 0;
      background: #fafafa;
    }
  </style>
</head>
<body>

<h1>企業情報クローリングシステム 基本設計書</h1>

<p>
本書は、<strong>「企業情報をWebからクロールし、標準的なデータ構造に保存することで、複数企業を横断的に分析できるシステム」</strong>の基本設計内容をまとめたものです。  
要件定義で示された機能要件・非機能要件を踏まえ、システムアーキテクチャ・モジュール設計・データベース設計・処理フロー等を整理します。
</p>

<hr>

<h2>1. システム概要</h2>
<ul>
  <li>本システムは、指定された複数の企業情報（公式サイト、IR情報、プレスリリース等）をクローリングし、データベースに格納します。</li>
  <li>MVP（Minimum Viable Product）の範囲として、
    <ul>
      <li>約100社程度の日本上場企業を対象</li>
      <li>財務指標（売上高、営業利益、純利益）、基本情報（企業名、代表者、所在地等）、ニュース／プレスリリース情報を収集</li>
      <li>週1回～日1回程度のバッチ処理でデータ更新</li>
    </ul>
  </li>
  <li>データは主に分析用（競合比較、投資判断等）に提供され、将来的にはBIツールとの連携を想定しています。</li>
</ul>

<hr>

<h2>2. アーキテクチャ構成</h2>
<p>システム全体の構成を以下に示します。</p>

<div class="mermaid">
flowchart LR
    A[クローラー<br>(スクレイピング/取得)] --> B[データクレンジング<br>バリデーション]
    B --> C[データベース<br>(RDB)]
    C --> D[BIツール/CSV出力<br>分析環境]
    B -->|エラーログ| E[監視/通知]
    A -->|定期ジョブ| E
</div>

<ul>
  <li><strong>クローラー：</strong>企業公式サイトやプレスリリース配信サイトへアクセスし、データ収集を行うモジュール。</li>
  <li><strong>データクレンジング・バリデーション：</strong>取得した生データを整形・標準化し、不正値や重複をチェックしてDBに格納。</li>
  <li><strong>データベース：</strong>主にリレーショナルDB（PostgreSQL等）を利用。<br>
  Company / Financial / News などのテーブル構造を持ち、更新の履歴を保持。</li>
  <li><strong>分析環境：</strong>BIツールへの連携やCSVエクスポートなどを通じて分析者にデータを提供。</li>
  <li><strong>監視・通知：</strong>クローリング失敗やデータ不備が発生した場合にアラートを上げる仕組み。</li>
</ul>

<hr>

<h2>3. クローリングモジュール設計</h2>
<p>クローリング処理の概要フローは以下のとおりです。</p>

<div class="mermaid">
sequenceDiagram
    participant Scheduler as 定期ジョブ(Scheduler)
    participant Crawler as クローラー
    participant Sites as 対象サイト
    participant DB as データベース
    participant Logger as ログ/通知

    Scheduler->>Crawler: 処理開始(週1回・日1回など)
    Crawler->>Sites: HTTPリクエスト/スクレイピング(API/RSS/HTML)
    Sites-->>Crawler: 取得データ(HTML, JSONなど)
    Crawler->>Crawler: データ解析(パース/タグ抽出)
    Crawler->>DB: データ挿入・更新(クレンジング/重複確認)
    Crawler->>Logger: ステータス送信(成功/失敗/エラー詳細)
    Logger-->>Scheduler: 結果通知
</div>

<h3>3.1 クローラーの処理</h3>
<ul>
  <li><strong>入力：</strong>対象企業リスト（URL、企業IDなど）、過去取得日時</li>
  <li><strong>手法：</strong>Pythonなどのスクリプトで、requests + BeautifulSoupやPlaywrightを使用。</li>
  <li><strong>レートリミット：</strong>サイトに過負荷をかけないよう適宜スリープを挟む。robots.txtの確認。</li>
  <li><strong>エラー処理：</strong>HTTPステータスエラー（429, 500等）は一定回数リトライし、失敗の場合はログ出力して後続処理を継続。</li>
</ul>

<hr>

<h2>4. データベースモジュール設計</h2>
<p>主なテーブル構成（ER図）を示します。</p>

<div class="mermaid">
erDiagram
    Company ||--o{ Financial : "1対多"
    Company ||--o{ News : "1対多"

    Company {
      string company_id PK
      string company_name
      string address
      date   founded_date
      string representative
      string website_url
      ...
    }
    Financial {
      string financial_id PK
      string company_id FK
      date   report_date
      float  sales
      float  operating_income
      float  net_income
      ...
    }
    News {
      string news_id PK
      string company_id FK
      date   published_date
      string title
      text   summary
      string source_url
      ...
    }
</div>

<h3>4.1 テーブル詳細</h3>
<ul>
  <li><strong>Companyテーブル</strong>
    <ul>
      <li>企業ID (company_id): 主キー</li>
      <li>企業名 (company_name)</li>
      <li>所在地 (address)</li>
      <li>設立日 (founded_date)</li>
      <li>代表者 (representative)</li>
      <li>ウェブサイトURL (website_url)</li>
    </ul>
  </li>
  <li><strong>Financialテーブル</strong>
    <ul>
      <li>財務ID (financial_id): 主キー</li>
      <li>企業ID (company_id): Companyへの外部キー</li>
      <li>決算日/報告日 (report_date)</li>
      <li>売上高 (sales)</li>
      <li>営業利益 (operating_income)</li>
      <li>純利益 (net_income)</li>
    </ul>
  </li>
  <li><strong>Newsテーブル</strong>
    <ul>
      <li>ニュースID (news_id): 主キー</li>
      <li>企業ID (company_id): Companyへの外部キー</li>
      <li>公開日 (published_date)</li>
      <li>タイトル (title)</li>
      <li>要約/本文 (summary)</li>
      <li>ソースURL (source_url)</li>
    </ul>
  </li>
</ul>

<hr>

<h2>5. データフロー設計</h2>
<p>データの流れをまとめます。</p>
<ol>
  <li><strong>企業リスト登録</strong><br>
    管理者がDBに企業リスト（company_id, website_url等）を登録。  
  </li>
  <li><strong>クローリング実行</strong><br>
    定期ジョブ（cron、Airflow、Lambdaなど）によりスクリプトを起動。  
  </li>
  <li><strong>スクレイピング・API取得</strong><br>
    企業公式サイト、IRページ、プレスリリース配信サイトを巡回し、HTMLやJSONを取得。  
  </li>
  <li><strong>データクレンジング／バリデーション</strong><br>
    重複チェック、文字化け除去、日付フォーマット統一、数値項目の単位揃えなどを行う。  
  </li>
  <li><strong>DB保存</strong><br>
    Company / Financial / News 各テーブルに挿入または更新。<br>
    既に存在するレコード（ニュースID、決算期が同一）の場合は更新扱い。  
  </li>
  <li><strong>分析用データ出力</strong><br>
    シンプルなダッシュボードやBIツールへのインポート、CSV出力を想定。  
  </li>
</ol>

<hr>

<h2>6. アプリケーション構成と処理詳細</h2>
<ul>
  <li><strong>アプリケーション層：</strong>
    <ul>
      <li>ジョブスケジューラ（cron / Airflowなど）：クローリング開始トリガー</li>
      <li>クローラー（Pythonスクリプト等）：対象サイトへのアクセス、HTML/API取得</li>
      <li>データクレンジングロジック：フォーマット整形、ユニット別変換、バリデーション</li>
    </ul>
  </li>
  <li><strong>DBアクセス層：</strong>
    <ul>
      <li>ORM（SQLAlchemy等）または生SQLでのDB操作</li>
      <li>Company, Financial, NewsテーブルへのINSERT/UPDATE/SELECT</li>
    </ul>
  </li>
  <li><strong>分析・レポート層：</strong>
    <ul>
      <li>BIツール用のデータエクスポート</li>
      <li>簡易的なダッシュボード（Webアプリまたはスプレッドシート連携）</li>
    </ul>
  </li>
</ul>

<hr>

<h2>7. 非機能要件（再掲と設計観点）</h2>
<ul>
  <li><strong>パフォーマンス：</strong>100社程度、1日1回のクロールであれば単一サーバー（またはLambda）のバッチでも可。</li>
  <li><strong>スケーラビリティ：</strong>対象企業拡大に応じて分散クローリング、キューイングの導入を検討。</li>
  <li><strong>セキュリティ：</strong>認証済みサーバー環境、DBのアクセス制限、HTTPS通信。</li>
  <li><strong>監視・運用：</strong>スクレイピングの成否を定期的にレポート。レイアウト変更検知の仕組み（エラー多発時にアラート）。</li>
  <li><strong>可用性：</strong>多少のクローリング失敗はリトライでカバー。クリティカルなダウンタイムは厳格に想定しないMVP構成。</li>
</ul>

<hr>

<h2>8. ログ／監視／障害対応</h2>
<ul>
  <li><strong>ログ取得：</strong>
    <ul>
      <li>クローリング開始・終了日時、取得成功数・失敗数</li>
      <li>例外発生時のエラーログ（スクリーンショット取得はオプション）</li>
    </ul>
  </li>
  <li><strong>監視方法：</strong>
    <ul>
      <li>AWS CloudWatch、DataDog、Elasticsearchなどを利用しエラーを検知</li>
      <li>Slack, Emailによる通知</li>
    </ul>
  </li>
  <li><strong>障害対応フロー：</strong>
    <ul>
      <li>定期ジョブが失敗 → ログ確認 → 手動リトライ</li>
      <li>サイトレイアウト変更 → スクリプト修正 → リリース</li>
    </ul>
  </li>
</ul>

<hr>

<h2>9. 今後の拡張ポイント</h2>
<ul>
  <li>ESG情報や特許情報の取得、子会社／関連会社情報の追加</li>
  <li>PDFの自動解析（有価証券報告書やアナリストレポート）</li>
  <li>大規模データレイク構築（S3 + Glue + Athenaなど）</li>
  <li>自然言語処理（ニュース本文からキーワード抽出、感情分析など）</li>
  <li>ダッシュボードの高度化（BIツールとの連携、リアルタイム分析）</li>
</ul>

<hr>

<h2>10. 変更履歴</h2>
<table>
  <thead>
    <tr>
      <th>バージョン</th>
      <th>日付</th>
      <th>変更内容</th>
      <th>作成/更新者</th>
      <th>承認者</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.1 (Draft)</td>
      <td>YYYY-MM-DD</td>
      <td>基本設計書 初版作成</td>
      <td>作成者名</td>
      <td>承認者名</td>
    </tr>
  </tbody>
</table>

<p class="note">
※本基本設計書は、MVP開発を想定した構成です。運用実績や要件変更に応じて改訂版を作成し、設計内容をアップデートしていきます。
</p>

</body>
</html>
